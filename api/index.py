
import os
import google.generativeai as genai
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import json

# --- CONFIGURATION ---
# Check if running in Vercel (environment variable usually present or we can infer)
# For Vercel, we might need root_path="/api" if the rewrite handles it that way.
# However, Vercel Serverless often strips the prefix before handing to WSGI?
# Actually, vercel.json rewrite "/api/(.*)" -> "/api/index.py"
# The ASGI app receives the full path including /api prefix usually.
# Safest bet: handle both or use root_path.

app = FastAPI(
    docs_url="/api/docs", 
    openapi_url="/api/openapi.json"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API Key Check
api_key = os.getenv("GEMINI_API_KEY") 
if api_key:
    genai.configure(api_key=api_key)
else:
    print("Warning: GEMINI_API_KEY not set in environment variables.")

# --- MODELS ---
class Transaction(BaseModel):
    id: str
    amount: float
    category: str
    date: str
    type: str # 'income' or 'expense'

class ChatRequest(BaseModel):
    message: str
    history: List[dict] = []
    context: Optional[str] = ""

class AnalysisRequest(BaseModel):
    transactions: List[Transaction]
    user_goal: Optional[str] = None

class CommandRequest(BaseModel):
    command: str
    current_date: str

# --- HELPER FUNCTIONS ---
def get_generative_model(system_instruction=None):
    try:
        if not api_key:
            raise Exception("API Key missing")
            
        model_name = 'gemini-1.5-flash' # Default fast model
        
        # Simple instantiation
        if system_instruction:
            return genai.GenerativeModel(model_name, system_instruction=system_instruction)
        return genai.GenerativeModel(model_name)
    except Exception as e:
        print(f"Model Init Error: {e}")
        # Fallback
        return genai.GenerativeModel('gemini-pro')

# --- LOGIC WITHOUT PANDAS (Optimized for Vercel) ---
def analyze_spending_logic(transactions_data):
    try:
        if not transactions_data:
            return {"insight": "Ch∆∞a c√≥ d·ªØ li·ªáu giao d·ªãch ƒë·ªÉ ph√¢n t√≠ch.", "actions": []}

        # Native Python Statistics
        expenses = [t for t in transactions_data if t.type == 'expense']
        
        if not expenses:
            return {"insight": "B·∫°n ch∆∞a c√≥ kho·∫£n chi ti√™u n√†o.", "actions": ["H√£y ghi ch√©p chi ti√™u ƒë·∫ßu ti√™n!"]}

        total_spent = sum(t.amount for t in expenses)
        
        # Group by category
        category_map = {}
        for t in expenses:
            category_map[t.category] = category_map.get(t.category, 0) + t.amount
            
        # Sort desc
        sorted_categories = sorted(category_map.items(), key=lambda item: item[1], reverse=True)
        
        top_category = sorted_categories[0][0]
        top_amount = sorted_categories[0][1]
        
        category_summary_str = "\n".join([f"- {cat}: {amt:,.0f}" for cat, amt in sorted_categories])

        # Ask Gemini
        prompt = f"""
        T√¥i l√† m·ªôt tr·ª£ l√Ω t√†i ch√≠nh c√° nh√¢n. Ng∆∞·ªùi d√πng ƒë√£ chi ti√™u t·ªïng c·ªông {total_spent:,.0f} VND.
        Danh m·ª•c t·ªën k√©m nh·∫•t l√† '{top_category}' v·ªõi {top_amount:,.0f} VND.
        Chi ti·∫øt:
        {category_summary_str}
        
        H√£y ƒë∆∞a ra 1 nh·∫≠n x√©t ng·∫Øn g·ªçn (d∆∞·ªõi 50 t·ª´) v√† 3 h√†nh ƒë·ªông ti·∫øt ki·ªám th·ª±c t·∫ø.
        Output JSON: {{ "insight": "...", "actions": [...] }}
        """
        
        model = get_generative_model()
        response = model.generate_content(prompt)
        text = response.text.replace('```json', '').replace('```', '').strip()
        return json.loads(text)
    except Exception as e:
        print(f"Analysis Logic Error: {e}")
        return {
            "insight": "Kh√¥ng th·ªÉ ph√¢n t√≠ch v√†o l√∫c n√†y (L·ªói Backend/API Key).",
            "actions": ["Ki·ªÉm tra c·∫•u h√¨nh API Key tr√™n Vercel"]
        }

def chat_advisor_logic(message: str, history: list = [], context: str = ""):
    try:
        system_instruction = """
        B·∫°n l√† **SmartLife AI** - Tr·ª£ l√Ω ·∫£o si√™u th√¥ng minh.
        Nhi·ªám v·ª•: Tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¥ng minh, h·ªØu √≠ch. D√πng Emoji üåü.
        N·∫øu c√≥ ng·ªØ c·∫£nh t√†i ch√≠nh, h√£y t∆∞ v·∫•n s√°t s∆∞·ªùn.
        """
        
        model = get_generative_model(system_instruction=system_instruction)
        
        # Simple history mapping
        gemini_history = []
        for msg in history[-5:]: # Keep context small
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content", "")
            if content:
                gemini_history.append({"role": role, "parts": [content]})

        chat = model.start_chat(history=gemini_history)
        
        user_message = message
        if context:
            user_message = f"[CONTEXT]: {context}\n\n[QUESTION]: {message}"
            
        response = chat.send_message(user_message)
        return response.text
    except Exception as e:
        print(f"Chat Error: {e}")
        return "Xin l·ªói, AI ƒëang b·∫≠n ho·∫∑c ch∆∞a c·∫•u h√¨nh ƒë√∫ng API Key. (H√£y ki·ªÉm tra Env Variable)"

def parse_schedule_logic(command: str, current_date: str):
    try:
        prompt = f"""
        Current Date: {current_date}
        Command: "{command}"
        Extract schedule event: title, start_time (HH:MM), end_time (HH:MM), day_of_week (0-6).
        Return JSON ONLY: {{ "title": "...", "start_time": "...", "end_time": "...", "day_of_week": int, "location": null }}
        If invalid, return {{ "error": "Invalid command" }}
        """
        model = get_generative_model()
        response = model.generate_content(prompt)
        text = response.text.replace('```json', '').replace('```', '').strip()
        return json.loads(text)
    except Exception as e:
        print(f"Schedule Parse Error: {e}")
        return {"error": "L·ªói x·ª≠ l√Ω AI"}

# --- ROUTES ---

@app.get("/api/health")
def health_check():
    return {"status": "ok", "environment": "Vercel"}

@app.post("/api/chat_finance")
async def chat_finance(req: ChatRequest):
    return {"response": chat_advisor_logic(req.message, req.history, req.context)}

@app.post("/api/analyze_finance")
async def analyze_finance(req: AnalysisRequest):
    return analyze_spending_logic(req.transactions)

@app.post("/api/parse_schedule")
async def parse_schedule(req: CommandRequest):
    return parse_schedule_logic(req.command, req.current_date)

# Fallback for local testing if running this file directly
if __name__ == "__main__":
    import uvicorn
    # When running locally, we might not have /api prefix in the URL if we hit root
    # But vite proxy sends /api.
    # To mimic vercel:
    uvicorn.run(app, host="0.0.0.0", port=8000)
