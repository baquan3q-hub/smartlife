import os
import sys

# Add the parent directory to sys.path to allow imports from smart_backend if needed
# But better to just copy the logic or import from smart_backend relative?
# The user's structures are a bit mixed. smart_backend is at root level sibling to src.
# To import from smart_backend from src/main.py:
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'smart_backend'))

import google.generativeai as genai
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv

# 1. C·∫•u h√¨nh
load_dotenv('.env.local')
load_dotenv() # Fallback

api_key = os.getenv("VITE_GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

app = FastAPI()

# 2. C·∫•u h√¨nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    print(">>> SERVER STARTED AT http://localhost:8000 <<<")
    print(f">>> API Key configured: {'YES' if api_key else 'NO'} <<<")
    print(">>> Available endpoints: /chat_finance, /analyze_finance <<<")
    try:
        print(">>> Checking available Gemini models... <<<")
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                print(f"   - {m.name}")
    except Exception as e:
        print(f">>> Error listing models: {e} <<<")

# --- MODELS ---
class Transaction(BaseModel):
    id: str
    amount: float
    category: str
    date: str
    type: str # 'income' or 'expense'

class AnalysisRequest(BaseModel):
    transactions: List[Transaction]
    user_goal: Optional[str] = None

class ChatRequest(BaseModel):
    message: str
    history: List[dict] = []
    context: Optional[str] = ""

class CommandRequest(BaseModel):
    command: str
    current_date: str

# --- ROUTES ---

@app.get("/")
def home():
    return {"status": "AI Backend is running (from src/main.py)!"}

# --- HELPER FUNCTIONS ---

def get_generative_model(system_instruction=None):
    """
    Dynamically attempts to find a working model from the user's list.
    Prioritizes Flash -> Pro -> Standard Gemini.
    """
    try:
        # Standard preferred models
        preferred_models = ['models/gemini-1.5-flash', 'models/gemini-1.5-pro', 'models/gemini-pro']
        
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        
        selected_model_name = None
        
        # 1. Check preferred
        for pref in preferred_models:
            if pref in available_models:
                selected_model_name = pref
                break
        
        # 2. Fallback to first available if no preferred found
        if not selected_model_name and available_models:
            selected_model_name = available_models[0]
            print(f">>> Warning: Preferred models not found. Using fallback: {selected_model_name}")
            
        if not selected_model_name:
            raise Exception("No 'generateContent' models available for this API Key.")
            
        # 3. Initialize
        # Note: older models like gemini-pro might not support system_instruction in constructor
        # We handle this gracefully
        if '1.5' in selected_model_name and system_instruction:
             return genai.GenerativeModel(selected_model_name, system_instruction=system_instruction)
        else:
             # Fallback for models that might not support system_instruction or if none provided
             if system_instruction:
                 print(f">>> Note: Model {selected_model_name} might not support system_instruction, sending as prompt prefix.")
             return genai.GenerativeModel(selected_model_name)

    except Exception as e:
        print(f"Model Selection Error: {e}")
        # Absolute fallback to string literal if list_models fails (e.g. key restriction)
        return genai.GenerativeModel('gemini-1.5-flash') 

# --- INLINE AGENT LOGIC (To avoid import issues) ---

def analyze_spending_logic(transactions_data):
    try:
        import pandas as pd
        if not transactions_data:
            return {"insight": "Ch∆∞a c√≥ d·ªØ li·ªáu giao d·ªãch ƒë·ªÉ ph√¢n t√≠ch.", "actions": []}

        df = pd.DataFrame([t.dict() for t in transactions_data])
        df['amount'] = pd.to_numeric(df['amount'])
        
        expenses = df[df['type'] == 'expense']
        if expenses.empty:
            return {"insight": "B·∫°n ch∆∞a c√≥ kho·∫£n chi ti√™u n√†o.", "actions": ["H√£y ghi ch√©p chi ti√™u ƒë·∫ßu ti√™n!"]}

        total_spent = expenses['amount'].sum()
        category_group = expenses.groupby('category')['amount'].sum().sort_values(ascending=False)
        top_category = category_group.index[0]
        top_amount = category_group.iloc[0]

        # Ask Gemini
        prompt = f"""
        T√¥i l√† m·ªôt tr·ª£ l√Ω t√†i ch√≠nh c√° nh√¢n. Ng∆∞·ªùi d√πng ƒë√£ chi ti√™u t·ªïng c·ªông {total_spent:,.0f} VND.
        Danh m·ª•c t·ªën k√©m nh·∫•t l√† '{top_category}' v·ªõi {top_amount:,.0f} VND.
        D·ªØ li·ªáu chi ti·∫øt: {category_group.to_string()}
        H√£y ƒë∆∞a ra 1 nh·∫≠n x√©t ng·∫Øn g·ªçn (d∆∞·ªõi 50 t·ª´) v√† 3 h√†nh ƒë·ªông ti·∫øt ki·ªám.
        Output JSON: {{ "insight": "...", "actions": [...] }}
        """
        
        model = get_generative_model()
        response = model.generate_content(prompt)
        text = response.text.replace('```json', '').replace('```', '').strip()
        import json
        return json.loads(text)
    except Exception as e:
        print(f"Analysis Logic Error: {e}")
        return {
            "insight": "Hi·ªán t·∫°i ch∆∞a th·ªÉ ph√¢n t√≠ch chi ti·∫øt do l·ªói h·ªá th·ªëng.",
            "actions": ["Ki·ªÉm tra l·∫°i d·ªØ li·ªáu", "Th·ª≠ l·∫°i sau"]
        }

def chat_advisor_logic(message: str, history: list = [], context: str = ""):
    try:
        system_instruction = """
        B·∫°n l√† **SmartLife AI** - Tr·ª£ l√Ω ·∫£o si√™u th√¥ng minh v√† s·∫Øc b√©n. üß†‚ú®
        
        **Nhi·ªám v·ª• c·ªßa b·∫°n:**
        1. **Tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi:** B·∫°n c√≥ ki·∫øn th·ª©c r·ªông v·ªÅ t√†i ch√≠nh, ƒë·ªùi s·ªëng, h·ªçc t·∫≠p v√† ph√°t tri·ªÉn b·∫£n th√¢n.
        2. **Ph√¢n t√≠ch s·∫Øc b√©n:** ƒê∆∞a ra l·∫≠p lu·∫≠n ch·∫∑t ch·∫Ω, r√†nh m·∫°ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ. Kh√¥ng tr·∫£ l·ªùi chung chung.
        3. **C√° nh√¢n h√≥a t·ªëi ƒëa:** D·ª±a v√†o [D·ªÆ LI·ªÜU NG·ªÆ C·∫¢NH] ƒë∆∞·ª£c cung c·∫•p (s·ªë d∆∞, chi ti√™u, th√≥i quen...) ƒë·ªÉ t∆∞ v·∫•n s√°t s∆∞·ªùn nh·∫•t.
        4. **Tr√¨nh b√†y ƒë·∫πp m·∫Øt:**
           - Lu√¥n s·ª≠ d·ª•ng **Emoji** üåü ph√π h·ª£p ƒë·ªÉ b√†i vi·∫øt sinh ƒë·ªông.
           - D√πng Markdown (In ƒë·∫≠m, G·∫°ch ƒë·∫ßu d√≤ng) ƒë·ªÉ chia √Ω r√µ r√†ng.
        
        **Phong c√°ch:** Th√¥ng minh, h√†i h∆∞·ªõc m·ªôt ch√∫t nh∆∞ng r·∫•t chuy√™n nghi·ªáp v√† ƒë√°ng tin c·∫≠y.
        """
        
        model = get_generative_model(system_instruction=system_instruction)
        
        # Basic history conversion (Limit to last 10 messages for speed)
        gemini_history = [{"role": "user" if msg["role"] == "user" else "model", "parts": [msg["content"]]} for msg in history[-10:]]
        
        # Start chat
        chat = model.start_chat(history=gemini_history)
        
        user_message = message
        if context:
            user_message = f"""
            [D·ªÆ LI·ªÜU NG·ªÆ C·∫¢NH C·ª¶A NG∆Ø·ªúI D√ôNG]:
            {context}
            
            [C√ÇU H·ªéI]:
            {message}
            """
            
        # Some older models/libraries might behave differently with start_chat, 
        # but standard genai logic usually holds.
        response = chat.send_message(user_message)
        return response.text
    except Exception as e:
        print(f"Chat Logic Error: {e}")
        # Fallback manual generation content if chat session fails
        try:
             model = get_generative_model()
             full_prompt = f"{system_instruction}\n\nHistory: {history}\n\nUser: {message}"
             res = model.generate_content(full_prompt)
             return res.text
        except:
             return "Xin l·ªói, t√¥i ƒëang g·∫∑p tr·ª•c tr·∫∑c k·∫øt n·ªëi model. B·∫°n th·ª≠ l·∫°i sau nh√©! üòì"

def parse_schedule_logic(command: str, current_date: str):
    try:
        # Scheduler Prompt
        prompt = f"""
        Current Date: {current_date}
        User Command: "{command}"

        Task: Extract the schedule event details from the command.
        1. If it's a valid task/event, return a JSON object with:
           - title: (string) Short summary
           - start_time: (string) HH:MM format (24h)
           - end_time: (string) HH:MM format (guess duration if not specified, default 1 hour)
           - day_of_week: (int) 0=Sunday, 1=Monday, ..., 6=Saturday. Calculate based on Current Date.
           - location: (string or null)
        
        2. If the command involves a specific date (e.g. "Next Friday"), calculate the correct 'day_of_week'.
        3. If no time is specified, default to "08:00".
        4. If it's NOT a scheduling command, return {{ "error": "Not a schedule command" }}

        Example Input: "H·ªçc to√°n l√∫c 8h s√°ng mai" (Assuming today is Monday)
        Example Output: {{ "title": "H·ªçc To√°n", "start_time": "08:00", "end_time": "09:00", "day_of_week": 2, "location": null }}

        Return ONLY the JSON string.
        """
        
        model = get_generative_model()
        response = model.generate_content(prompt)
        text = response.text.replace('```json', '').replace('```', '').strip()
        import json
        return json.loads(text)
    except Exception as e:
        print(f"Scheduler Logic Error: {e}")
        return {"error": "Kh√¥ng th·ªÉ hi·ªÉu l·ªánh n√†y. Vui l√≤ng th·ª≠ l·∫°i r√µ r√†ng h∆°n."}

# --- ROUTES ---

@app.post("/chat_finance")
async def chat_finance(req: ChatRequest):
    print(f"Chat Request: {req.message}")
    response = chat_advisor_logic(req.message, req.history, req.context)
    return {"response": response}

@app.post("/analyze_finance")
async def analyze_finance(req: AnalysisRequest):
    print(f"Analyze Request: {len(req.transactions)} txns")
    return analyze_spending_logic(req.transactions)

@app.post("/parse_schedule")
async def parse_schedule(req: CommandRequest):
    print(f"Schedule Request: {req.command}")
    return parse_schedule_logic(req.command, req.current_date)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)